{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCh9BTedHJK1"
   },
   "source": [
    "![pageindex_banner](https://pageindex.ai/static/images/pageindex_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nD0hb4TFHWTt"
   },
   "source": [
    "<p align=\"center\"><i>Reasoning-based RAG&nbsp; ‚úß &nbsp;No Vector DB&nbsp; ‚úß &nbsp;No Chunking&nbsp; ‚úß &nbsp;Human-like Retrieval</i></p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"https://vectify.ai\">üè† Homepage</a>&nbsp; ‚Ä¢ &nbsp;\n",
    "  <a href=\"https://dash.pageindex.ai\">üñ•Ô∏è Dashboard</a>&nbsp; ‚Ä¢ &nbsp;\n",
    "  <a href=\"https://docs.pageindex.ai/quickstart\">üìö API Docs</a>&nbsp; ‚Ä¢ &nbsp;\n",
    "  <a href=\"https://github.com/VectifyAI/PageIndex\">üì¶ GitHub</a>&nbsp; ‚Ä¢ &nbsp;\n",
    "  <a href=\"https://discord.com/invite/VuXuf29EUj\">üí¨ Discord</a>&nbsp; ‚Ä¢ &nbsp;\n",
    "  <a href=\"https://ii2abc2jejf.typeform.com/to/tK3AXl8T\">‚úâÔ∏è Contact</a>&nbsp;\n",
    "</p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "[![Star us on GitHub](https://img.shields.io/github/stars/VectifyAI/PageIndex?style=for-the-badge&logo=github&label=‚≠êÔ∏è%20Star%20Us)](https://github.com/VectifyAI/PageIndex) &nbsp;&nbsp; [![Follow us on X](https://img.shields.io/badge/Follow%20Us-000000?style=for-the-badge&logo=x&logoColor=white)](https://twitter.com/VectifyAI)\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ebvn5qfpcG1K"
   },
   "source": [
    "# Simple Vectorless RAG with PageIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageIndex Introduction\n",
    "PageIndex is a new **reasoning-based**, **vectorless RAG** framework that performs retrieval in two steps:  \n",
    "1. Generate a tree structure index of documents  \n",
    "2. Perform reasoning-based retrieval through tree search  \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://docs.pageindex.ai/images/cookbook/vectorless-rag.png\" width=\"70%\">\n",
    "</div>\n",
    "\n",
    "Compared to traditional vector-based RAG, PageIndex features:\n",
    "- **No Vectors Needed**: Uses document structure and LLM reasoning for retrieval.\n",
    "- **No Chunking Needed**: Documents are organized into natural sections rather than artificial chunks.\n",
    "- **Human-like Retrieval**: Simulates how human experts navigate and extract knowledge from complex documents. \n",
    "- **Transparent Retrieval Process**: Retrieval based on reasoning ‚Äî say goodbye to approximate semantic search (\"vibe retrieval\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Notebook Overview\n",
    "\n",
    "This notebook demonstrates a simple, minimal example of **vectorless RAG** with PageIndex. You will learn how to:\n",
    "- [x] Build a PageIndex tree structure of a document\n",
    "- [x] Perform reasoning-based retrieval with tree search\n",
    "- [x] Generate answers based on the retrieved context\n",
    "\n",
    "> ‚ö° Note: This is a **minimal example** to illustrate PageIndex's core philosophy and idea, not its full capabilities. More advanced examples are coming soon.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ziuTbbWcG1L"
   },
   "source": [
    "## Step 0: Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edTfrizMFK4c"
   },
   "source": [
    "#### 0.1 Install PageIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install packages (run once)\n",
    "%pip install -q llama-cpp-python pageindex nest-asyncio\n",
    "%pip install -q --upgrade pageindex\n",
    "# Imports\n",
    "from pageindex import PageIndexClient\n",
    "import pageindex.utils as utils\n",
    "from llama_cpp import Llama\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Enable nested async (needed for Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVEWzPKGcG1M"
   },
   "source": [
    "#### 0.2 Setup PageIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PageIndex client initialized!\n"
     ]
    }
   ],
   "source": [
    "# Get your PageIndex API key from https://dash.pageindex.ai/api-keys\n",
    "PAGEINDEX_API_KEY = \"61c71223516242df905cf6e1af28aa29\"\n",
    "pi_client = PageIndexClient(api_key=PAGEINDEX_API_KEY)\n",
    "\n",
    "print(\"‚úÖ PageIndex client initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 Setup LLM\n",
    "\n",
    "Choose your preferred LLM for reasoning-based retrieval. In this example, we use OpenAI‚Äôs GPT-4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "StvqfcK4cG1M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LM Studio server is UP!\n",
      "Models: {'data': [{'id': 'meta-llama-llama-3.1-8b-instruct', 'object': 'model', 'owned_by': 'organization_owner'}, {'id': 'text-embedding-nomic-embed-text-v1.5', 'object': 'model', 'owned_by': 'organization_owner'}], 'object': 'list'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://127.0.0.1:1234/v1/models\", timeout=5)\n",
    "    print(\"‚úÖ LM Studio server is UP!\")\n",
    "    print(f\"Models: {response.json()}\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå LM Studio server is NOT running!\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nüëâ Start it in LM Studio app ‚Üí Local Server ‚Üí Start Server\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LM Studio function ready!\n",
      "‚úÖ Test Response: {\n",
      "        \"thinking\": \"testing\",\n",
      "        \"node_list\": [\"0019\"]\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "# ===== FINAL BULLETPROOF VERSION =====\n",
    "from openai import OpenAI\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import re\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",  # Add /v1 at the end\n",
    "    api_key=\"not-needed\"\n",
    ")\n",
    "\n",
    "async def call_llm(prompt, temperature=0):\n",
    "    \"\"\"\n",
    "    Call LM Studio - bulletproof version\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    def _call():\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"local-model\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=4096\n",
    "            )\n",
    "            \n",
    "            # Safely get text\n",
    "            if response and response.choices and len(response.choices) > 0:\n",
    "                raw_text = response.choices[0].message.content\n",
    "                if raw_text:\n",
    "                    raw_text = raw_text.strip()\n",
    "                    \n",
    "                    # Extract JSON if present\n",
    "                    json_match = re.search(r'\\{.*?\\}', raw_text, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        return json_match.group(0)\n",
    "                    \n",
    "                    return raw_text\n",
    "            \n",
    "            return \"No response from model\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Full error: {traceback.format_exc()}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    result = await loop.run_in_executor(None, _call)\n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ LM Studio function ready!\")\n",
    "\n",
    "# Test\n",
    "test_result = await call_llm('Return just: {\"thinking\": \"testing\", \"node_list\": [\"0019\"]}')\n",
    "print(f\"‚úÖ Test Response: {test_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heGtIMOVcG1N"
   },
   "source": [
    "## Step 1: PageIndex Tree Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mzd1VWjwMUJL"
   },
   "source": [
    "#### 1.1 Submit a document for generating PageIndex tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6--eZPLcG1N",
    "outputId": "ca688cfd-6c4b-4a57-dac2-f3c2604c4112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Downloaded https://arxiv.org/pdf/2501.12948.pdf\n",
      "‚úÖ Document Submitted: pi-cmh4z8cfq05bd0cr1zqzi0igk\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "\n",
    "# Download PDF\n",
    "pdf_url = \"https://arxiv.org/pdf/2501.12948.pdf\"\n",
    "pdf_path = os.path.join(\"./data\", pdf_url.split('/')[-1])\n",
    "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
    "\n",
    "response = requests.get(pdf_url)\n",
    "with open(pdf_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "print(f\"‚úÖ Downloaded {pdf_url}\")\n",
    "\n",
    "# Submit to PageIndex (uses their API for tree building)\n",
    "doc_id = pi_client.submit_document(pdf_path)[\"doc_id\"]\n",
    "print(f'‚úÖ Document Submitted: {doc_id}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-Hrh0azcG1N"
   },
   "source": [
    "#### 1.2 Get the generated PageIndex tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b1Q1g6vrcG1O",
    "outputId": "dc944660-38ad-47ea-d358-be422edbae53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Processing document...\n",
      "‚è≥ Processing document...\n",
      "‚è≥ Processing document...\n",
      "‚è≥ Processing document...\n",
      "‚úÖ Simplified Tree Structure:\n",
      "[{'title': 'DeepSeek-R1: Incentivizing Reasoning Cap...',\n",
      "  'node_id': '0000',\n",
      "  'prefix_summary': '# DeepSeek-R1: Incentivizing Reasoning C...',\n",
      "  'nodes': [{'title': 'Abstract',\n",
      "             'node_id': '0001',\n",
      "             'summary': 'The text introduces DeepSeek-R1-Zero, a ...'},\n",
      "            {'title': 'Contents',\n",
      "             'node_id': '0002',\n",
      "             'summary': 'This document outlines an approach invol...'},\n",
      "            {'title': '1. Introduction',\n",
      "             'node_id': '0003',\n",
      "             'prefix_summary': 'This paper introduces a novel approach t...',\n",
      "             'nodes': [{'title': '1.1. Contributions',\n",
      "                        'node_id': '0004',\n",
      "                        'summary': '### 1.1. Contributions\\n'},\n",
      "                       {'title': 'Post-Training: Large-Scale Reinforcement...',\n",
      "                        'node_id': '0005',\n",
      "                        'summary': 'The text describes two approaches to dev...'},\n",
      "                       {'title': 'Distillation: Smaller Models Can Be Powe...',\n",
      "                        'node_id': '0006',\n",
      "                        'summary': 'This text demonstrates that reasoning pa...'},\n",
      "                       {'title': '1.2. Summary of Evaluation Results',\n",
      "                        'node_id': '0007',\n",
      "                        'summary': 'The text summarizes the evaluation resul...'}]},\n",
      "            {'title': '2. Approach',\n",
      "             'node_id': '0008',\n",
      "             'prefix_summary': '## 2. Approach\\n',\n",
      "             'nodes': [{'title': '2.1. Overview',\n",
      "                        'node_id': '0009',\n",
      "                        'summary': '### 2.1. Overview\\n\\nPrevious work has hea...'},\n",
      "                       {'title': '2.2. DeepSeek-R1-Zero: Reinforcement Lea...',\n",
      "                        'node_id': '0010',\n",
      "                        'prefix_summary': '### 2.2. DeepSeek-R1-Zero: Reinforcement...',\n",
      "                        'nodes': [{'title': '2.2.1. Reinforcement Learning Algorithm',\n",
      "                                   'node_id': '0011',\n",
      "                                   'summary': 'The text describes the Group Relative Po...'},\n",
      "                                  {'title': '2.2.2. Reward Modeling',\n",
      "                                   'node_id': '0012',\n",
      "                                   'summary': 'The text describes the rule-based reward...'},\n",
      "                                  {'title': '2.2.3. Training Template',\n",
      "                                   'node_id': '0013',\n",
      "                                   'summary': '#### 2.2.3. Training Template\\n\\nTo train ...'},\n",
      "                                  {'title': '2.2.4. Performance, Self-evolution Proce...',\n",
      "                                   'node_id': '0014',\n",
      "                                   'summary': 'The text details the performance, self-e...'}]},\n",
      "                       {'title': '2.3. DeepSeek-R1: Reinforcement Learning...',\n",
      "                        'node_id': '0015',\n",
      "                        'summary': 'The text describes the training pipeline...'},\n",
      "                       {'title': '2.4. Distillation: Empower Small Models ...',\n",
      "                        'node_id': '0016',\n",
      "                        'summary': 'This section details a distillation meth...'}]},\n",
      "            {'title': '3. Experiment',\n",
      "             'node_id': '0017',\n",
      "             'prefix_summary': 'The text details the experimental setup ...',\n",
      "             'nodes': [{'title': '3.1. DeepSeek-R1 Evaluation',\n",
      "                        'node_id': '0018',\n",
      "                        'summary': 'This text presents a detailed evaluation...'},\n",
      "                       {'title': '3.2. Distilled Model Evaluation',\n",
      "                        'node_id': '0019',\n",
      "                        'summary': 'Table 5 evaluates distilled DeepSeek-R1 ...'}]},\n",
      "            {'title': '4. Discussion',\n",
      "             'node_id': '0020',\n",
      "             'summary': 'The discussion compares distillation wit...'},\n",
      "            {'title': '5. Conclusion, Limitations, and Future W...',\n",
      "             'node_id': '0021',\n",
      "             'summary': 'This text concludes by presenting DeepSe...'},\n",
      "            {'title': 'References',\n",
      "             'node_id': '0022',\n",
      "             'summary': 'This text is a collection of references ...'},\n",
      "            {'title': 'Appendix', 'node_id': '0023', 'summary': '## Appendix\\n'},\n",
      "            {'title': 'A. Contributions and Acknowledgments',\n",
      "             'node_id': '0024',\n",
      "             'summary': 'The text provides a comprehensive list o...'}]}]\n"
     ]
    }
   ],
   "source": [
    "# Wait for processing\n",
    "import time\n",
    "while not pi_client.is_retrieval_ready(doc_id):\n",
    "    print(\"‚è≥ Processing document...\")\n",
    "    time.sleep(5)\n",
    "\n",
    "# Get tree structure\n",
    "tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
    "print('‚úÖ Simplified Tree Structure:')\n",
    "utils.print_tree(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USoCLOiQcG1O"
   },
   "source": [
    "## Step 2: Reasoning-Based Retrieval with Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Use LLM for tree search and identify nodes that might contain relevant context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "        \"thinking\": \"testing\",\n",
      "        \"node_list\": [\"0019\"]\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Return just: {\"thinking\": \"testing\", \"node_list\": [\"0019\"]}'\n",
    "result = await call_llm(prompt)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LLHNJAtTcG1O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching tree with local Llama...\n",
      "‚úÖ Search complete!\n",
      "{\n",
      "  \"thinking\": \"The question is asking for nodes that are likely to contain the answer to 'What are the conclusions in this document?' which suggests we need to find nodes related to summary or conclusion.\",\n",
      "  \"node_list\": [\"0002\", \"0017\", \"0021\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Your query\n",
    "query = \"What are the conclusions in this document?\"\n",
    "\n",
    "# Remove text fields to reduce prompt size\n",
    "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
    "\n",
    "# Search prompt\n",
    "search_prompt = f\"\"\"You are given a question and a tree structure of a document.\n",
    "Each node contains a node id, node title, and a corresponding summary.\n",
    "Your task is to find all nodes that are likely to contain the answer to the question.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Document tree structure:\n",
    "{json.dumps(tree_without_text, indent=2)}\n",
    "\n",
    "Please reply in the following JSON format:\n",
    "{{\n",
    "  \"thinking\": \"Your reasoning process\",\n",
    "  \"node_list\": [\"node_id_1\", \"node_id_2\", ...]\n",
    "}}\n",
    "\n",
    "Only return valid JSON, nothing else.\"\"\"\n",
    "\n",
    "print(\"üîç Searching tree with local Llama...\")\n",
    "tree_search_result = await call_llm(search_prompt, temperature=0)\n",
    "print(f\"‚úÖ Search complete!\")\n",
    "print(tree_search_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Print retrieved nodes and reasoning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "P8DVUOuAen5u",
    "outputId": "6bb6d052-ef30-4716-f88e-be98bcb7ebdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Reasoning Process:\n",
      "The question is asking for nodes that are likely to contain the answer to 'What are the conclusions\n",
      "in this document?' which suggests we need to find nodes related to summary or conclusion.\n",
      "\n",
      "üìÑ Retrieved Nodes:\n",
      "Node ID: 0002\t Page: 2\t Title: Contents\n",
      "Node ID: 0017\t Page: 11\t Title: 3. Experiment\n",
      "Node ID: 0021\t Page: 16\t Title: 5. Conclusion, Limitations, and Future Work\n"
     ]
    }
   ],
   "source": [
    "node_map = utils.create_node_mapping(tree)\n",
    "\n",
    "# Parse JSON response\n",
    "try:\n",
    "    tree_search_result_json = json.loads(tree_search_result)\n",
    "    \n",
    "    print('üß† Reasoning Process:')\n",
    "    utils.print_wrapped(tree_search_result_json.get('thinking', 'No reasoning provided'))\n",
    "    \n",
    "    print('\\nüìÑ Retrieved Nodes:')\n",
    "    for node_id in tree_search_result_json[\"node_list\"]:\n",
    "        node = node_map[node_id]\n",
    "        print(f\"Node ID: {node['node_id']}\\t Page: {node['page_index']}\\t Title: {node['title']}\")\n",
    "        \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"‚ö†Ô∏è JSON parsing error: {e}\")\n",
    "    print(\"Raw response:\", tree_search_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10wOZDG_cG1O"
   },
   "source": [
    "## Step 3: Answer Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Extract relevant context from retrieved nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "a7UCBnXlcG1O",
    "outputId": "8a026ea3-4ef3-473a-a57b-b4565409749e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Retrieved Context:\n",
      "\n",
      "## Contents\n",
      "\n",
      "1 Introduction ..... 3\n",
      "1.1 Contributions ..... 4\n",
      "1.2 Summary of Evaluation Results ..... 4\n",
      "2 Approach ..... 5\n",
      "2.1 Overview ..... 5\n",
      "2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model ..... 5\n",
      "2.2.1 Reinforcement Learning Algorithm ..... 5\n",
      "2.2.2 Reward Modeling ..... 6\n",
      "2.2.3 Training Template ..... 6\n",
      "2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ..... 6\n",
      "2.3 DeepSeek-R1: Reinforcement Learning with Cold Start ..... 9\n",
      "2.3.1 Cold Start ..... 9\n",
      "2.3.2 Reasoning-oriented Reinforcement Learning ..... 10\n",
      "2.3.3 Rejection Sampling and Supervised Fine-Tuning ..... 10\n",
      "2.3.4 Reinforcement Learning for all Scenarios ..... 11\n",
      "2.4 Distillation: Empower Small Models with Reasoning Capability ..... 11\n",
      "3 Experiment ..... 11\n",
      "3.1 DeepSeek-R1 Evaluation ..... 13\n",
      "3.2 Distilled Model Evaluation ..... 14\n",
      "4 Discussion ..... 14\n",
      "4.1 Distillation v.s. Reinforcement Learning ..... 14\n",
      "4.2 Unsuccessful Attempts ..... 15\n",
      "5 Conclusion, Limitations, and Future Work .....\n"
     ]
    }
   ],
   "source": [
    "node_list = json.loads(tree_search_result)[\"node_list\"]\n",
    "relevant_content = \"\\n\\n\".join(node_map[node_id][\"text\"] for node_id in node_list)\n",
    "\n",
    "print('üìñ Retrieved Context:\\n')\n",
    "utils.print_wrapped(relevant_content[:1000] + '...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Generate answer based on retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "tcp_PhHzcG1O",
    "outputId": "187ff116-9bb0-4ab4-bacb-13944460b5ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Generating Answer with Local Llama...\n",
      "\n",
      "The conclusions in this document are:\n",
      "\n",
      "1. DeepSeek-R1-Zero achieves strong performance across various tasks using a pure reinforcement\n",
      "learning approach.\n",
      "2. DeepSeek-R1 is more powerful and achieves performance comparable to OpenAI-o1-1217 on a range of\n",
      "tasks by leveraging cold-start data alongside iterative RL fine-tuning.\n",
      "3. Distillation of the reasoning capability from DeepSeek-R1 to small dense models results in\n",
      "promising outcomes, with one model outperforming GPT-4o and Claude-3.5-Sonnet on math benchmarks.\n"
     ]
    }
   ],
   "source": [
    "answer_prompt = f\"\"\"Answer the question based on the context:\n",
    "\n",
    "Question: {query}\n",
    "Context: {relevant_content}\n",
    "\n",
    "Provide a clear, concise answer based only on the context provided.\n",
    "\"\"\"\n",
    "\n",
    "print('üí° Generating Answer with Local Llama...\\n')\n",
    "answer = await call_llm(answer_prompt, temperature=0.3)\n",
    "utils.print_wrapped(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1kaGD3GcG1O"
   },
   "source": [
    "---\n",
    "\n",
    "## üéØ What's Next\n",
    "\n",
    "This notebook has demonstrated a **basic**, **minimal** example of **reasoning-based**, **vectorless** RAG with PageIndex. The workflow illustrates the core idea:\n",
    "> *Generating a hierarchical tree structure from a document, reasoning over that tree structure, and extracting relevant context, without relying on a vector database or top-k similarity search*.\n",
    "\n",
    "While this notebook highlights a minimal workflow, the PageIndex framework is built to support **far more advanced** use cases. In upcoming tutorials, we will introduce:\n",
    "* **Multi-Node Reasoning with Content Extraction** ‚Äî Scale tree search to extract and select relevant content from multiple nodes.\n",
    "* **Multi-Document Search** ‚Äî Enable reasoning-based navigation across large document collections, extending beyond a single file.\n",
    "* **Efficient Tree Search** ‚Äî Improve tree search efficiency for long documents with a large number of nodes.\n",
    "* **Expert Knowledge Integration and Preference Alignment** ‚Äî Incorporate user preferences or expert insights by adding knowledge directly into the LLM tree search, without the need for fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Learn More About PageIndex\n",
    "  <a href=\"https://vectify.ai\">üè† Homepage</a>&nbsp; ‚Ä¢ &nbsp;\n",
    "  <a href=\"https://dash.pageindex.ai\">üñ•Ô∏è Dashboard</a>&nbsp; ‚Ä¢ &nbsp;\n",
    "  <a href=\"https://docs.pageindex.ai/quickstart\">üìö API Docs</a>&nbsp; ‚Ä¢ &nbsp;\n",
    "  <a href=\"https://github.com/VectifyAI/PageIndex\">üì¶ GitHub</a>&nbsp; ‚Ä¢ &nbsp;\n",
    "  <a href=\"https://discord.com/invite/VuXuf29EUj\">üí¨ Discord</a>&nbsp; ‚Ä¢ &nbsp;\n",
    "  <a href=\"https://ii2abc2jejf.typeform.com/to/tK3AXl8T\">‚úâÔ∏è Contact</a>\n",
    "\n",
    "<br>\n",
    "\n",
    "¬© 2025 [Vectify AI](https://vectify.ai)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
